{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "This notebook trains and saves a machine learning model using an LSTM architecture for predicting stock prices. The focus is on individual tickers.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "1.\tLoad processed data from SQLite.\n",
    "\n",
    "2.\tFilter data for a specific ticker (default: XOM).\n",
    "\n",
    "3.\tPreprocess and normalize features.\n",
    "\n",
    "4.\tBuild and train an LSTM model.\n",
    "\n",
    "5.\tSave the trained LSTM model and scaler for use in evaluation and predictions.\n",
    "\n",
    "Import Libraries\n",
    "\n",
    "•\tpandas and sqlite3: For data manipulation and interaction with SQLite.\n",
    "\n",
    "•\tkeras and tensorflow: For building and training the LSTM model.\n",
    "\n",
    "•\tscikit-learn: For preprocessing and model evaluation.\n",
    "\n",
    "•\tjoblib: For saving trained models and scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import sqlite3\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Processed Data\n",
    "\n",
    "•\tLoad the preprocessed stock data from the SQLite database (stocks_data.db).\n",
    "\n",
    "•\tEnsure the dataset is ready for filtering and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data: 429528 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to SQLite database\n",
    "db_path = 'database/stocks_data.db'\n",
    "\n",
    "# Load data from SQLite\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    query = \"SELECT * FROM processed_stocks\"\n",
    "    data = pd.read_sql(query, conn)\n",
    "print(f\"Loaded processed data: {data.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Data for Default Ticker\n",
    "\n",
    "•\tFilter data to include only rows for the default ticker (e.g., XOM).\n",
    "\n",
    "•\tVerify the number of rows available for the selected ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for XOM: 75252 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Set default ticker\n",
    "default_ticker = 'XOM'\n",
    "\n",
    "# Step 2: Filter data for the default ticker\n",
    "ticker_data = data[data['Ticker'] == default_ticker]\n",
    "print(f\"Loaded data for {default_ticker}: {ticker_data.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Features and Target\n",
    "\n",
    "•\tDefine independent variables (features) for predictions: ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2'].\n",
    "\n",
    "•\tDefine the dependent variable (target): Adj Close.\n",
    "\n",
    "•\tPrepare the dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features and target\n",
    "features = ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2']\n",
    "target = 'Adj Close'\n",
    "\n",
    "X = ticker_data[features]\n",
    "y = ticker_data[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Features\n",
    "\n",
    "•\tUse MinMaxScaler to scale the features between 0 and 1.\n",
    "\n",
    "•\tFit the scaler on the training data and apply the transformation to both training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Training and Testing Sets\n",
    "\n",
    "•\tDivide the dataset into:\n",
    "\n",
    "•\tTraining Set: 80% of the data, used for model training.\n",
    "\n",
    "•\tTesting Set: 20% of the data, used for model evaluation.\n",
    "\n",
    "•\tEnsures fair evaluation of the model’s predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape Input for LSTM\n",
    "\n",
    "•\tReshape the input data to match the expected format for LSTM models: (samples, timesteps, features).\n",
    "\n",
    "•\tIn this case, timesteps=1 as each prediction is based on one timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape input for LSTM (samples, timesteps, features)\n",
    "X_train_scaled = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_scaled = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Compile the LSTM Model\n",
    "\n",
    "•\tModel Architecture:\n",
    "\n",
    "•\tFirst LSTM layer with 64 neurons, relu activation, and return_sequences=True to allow stacking.\n",
    "\n",
    "•\tDropout layer to reduce overfitting.\n",
    "\n",
    "•\tSecond LSTM layer with 32 neurons and another Dropout layer.\n",
    "\n",
    "•\tFully connected Dense output layer with 1 neuron for regression.\n",
    "\n",
    "•\tCompilation:\n",
    "\n",
    "•\tOptimizer: adam\n",
    "\n",
    "•\tLoss function: mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the optimized LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(1, X_train_scaled.shape[2])))\n",
    "model.add(Dropout(0.2))  # Add dropout for regularization\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Fully connected output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the LSTM Model\n",
    "\n",
    "•\tUse EarlyStopping to stop training if validation loss does not improve after 10 epochs.\n",
    "\n",
    "•\tTrain the model for up to 100 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1937.9607 - val_loss: 16.8313\n",
      "Epoch 2/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.3168 - val_loss: 10.1263\n",
      "Epoch 3/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.1432 - val_loss: 8.2314\n",
      "Epoch 4/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 61.9659 - val_loss: 6.0566\n",
      "Epoch 5/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 60.8176 - val_loss: 6.2303\n",
      "Epoch 6/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 58.9323 - val_loss: 3.7803\n",
      "Epoch 7/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 58.4664 - val_loss: 3.6954\n",
      "Epoch 8/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 55.6250 - val_loss: 2.9585\n",
      "Epoch 9/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 54.7621 - val_loss: 4.2714\n",
      "Epoch 10/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 53.2132 - val_loss: 2.7385\n",
      "Epoch 11/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 53.7637 - val_loss: 3.2477\n",
      "Epoch 12/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 50.9664 - val_loss: 3.3292\n",
      "Epoch 13/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 51.0319 - val_loss: 2.2210\n",
      "Epoch 14/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 50.9929 - val_loss: 3.4677\n",
      "Epoch 15/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 47.9950 - val_loss: 2.6958\n",
      "Epoch 16/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 45.8857 - val_loss: 3.3912\n",
      "Epoch 17/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 45.1093 - val_loss: 3.1372\n",
      "Epoch 18/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 44.9684 - val_loss: 1.8297\n",
      "Epoch 19/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 42.5030 - val_loss: 3.1917\n",
      "Epoch 20/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 42.4870 - val_loss: 1.7390\n",
      "Epoch 21/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 41.7856 - val_loss: 1.9516\n",
      "Epoch 22/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 41.4323 - val_loss: 2.5173\n",
      "Epoch 23/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 40.1214 - val_loss: 2.2949\n",
      "Epoch 24/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 39.8188 - val_loss: 1.5777\n",
      "Epoch 25/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.8349 - val_loss: 2.1621\n",
      "Epoch 26/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 38.4535 - val_loss: 2.3238\n",
      "Epoch 27/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 39.0607 - val_loss: 3.9019\n",
      "Epoch 28/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.6545 - val_loss: 1.6624\n",
      "Epoch 29/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 36.7146 - val_loss: 1.3833\n",
      "Epoch 30/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 36.6997 - val_loss: 2.0730\n",
      "Epoch 31/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.7250 - val_loss: 1.6802\n",
      "Epoch 32/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 35.8658 - val_loss: 1.3767\n",
      "Epoch 33/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.5367 - val_loss: 2.5156\n",
      "Epoch 34/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.8530 - val_loss: 1.4500\n",
      "Epoch 35/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 34.0423 - val_loss: 1.4870\n",
      "Epoch 36/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 32.2412 - val_loss: 1.5407\n",
      "Epoch 37/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.8670 - val_loss: 1.8193\n",
      "Epoch 38/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.4347 - val_loss: 1.3843\n",
      "Epoch 39/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.3766 - val_loss: 1.2703\n",
      "Epoch 40/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 30.3153 - val_loss: 1.0958\n",
      "Epoch 41/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 31.0090 - val_loss: 1.9189\n",
      "Epoch 42/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 29.3331 - val_loss: 1.9244\n",
      "Epoch 43/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 29.4614 - val_loss: 1.0663\n",
      "Epoch 44/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 29.7821 - val_loss: 0.9720\n",
      "Epoch 45/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 29.2998 - val_loss: 1.1919\n",
      "Epoch 46/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 28.6040 - val_loss: 2.1679\n",
      "Epoch 47/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.7697 - val_loss: 1.2821\n",
      "Epoch 48/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.5093 - val_loss: 1.4974\n",
      "Epoch 49/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.9420 - val_loss: 1.6543\n",
      "Epoch 50/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.2745 - val_loss: 1.1813\n",
      "Epoch 51/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27.1621 - val_loss: 1.6665\n",
      "Epoch 52/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 26.4594 - val_loss: 1.0840\n",
      "Epoch 53/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 26.1509 - val_loss: 1.2174\n",
      "Epoch 54/100\n",
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 25.7493 - val_loss: 0.9761\n"
     ]
    }
   ],
   "source": [
    "# Callbacks for better training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=100,  # Start with 100 epochs\n",
    "    batch_size=128,  # increased batch size\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model\n",
    "\n",
    "•\tUse the test data to evaluate the model’s performance:\n",
    "\n",
    "•\tMean Squared Error (MSE): Measures average squared differences between actual and predicted values.\n",
    "\n",
    "•\tMean Absolute Error (MAE): Measures the average magnitude of errors.\n",
    "\n",
    "•\tR-squared (R²): Proportion of variance explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m471/471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step\n",
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 0.97\n",
      "Mean Absolute Error (MAE): 0.66\n",
      "R-squared (R²): 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model and Scaler\n",
    "\n",
    "•\tSave the trained LSTM model as model_<TICKER>_lstm.h5.\n",
    "\n",
    "•\tSave the fitted scaler as scaler_<TICKER>_lstm.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOM model saved as 'models/model_XOM_lstm.h5'\n",
      "XOM scaler saved as 'models/scaler_XOM_lstm.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the trained model and scaler\n",
    "model_filename = f'models/model_{default_ticker}_lstm.h5'\n",
    "scaler_filename = f'models/scaler_{default_ticker}_lstm.pkl'\n",
    "\n",
    "model.save(model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"{default_ticker} model saved as '{model_filename}'\")\n",
    "print(f\"{default_ticker} scaler saved as '{scaler_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "\n",
    "1.\tExtend the Flask app to load LSTM models dynamically based on the selected ticker.\n",
    "\n",
    "2.\tCreate additional visualizations for residuals and predicted values.\n",
    "\n",
    "3.\tEvaluate the model’s performance across multiple tickers to ensure robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
