{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed data: 178922 rows\n",
      "Loaded data for XOM: 31347 rows\n",
      "Shape of X: (31337, 10, 5), Shape of y: (31337,)\n",
      "Updated feature set: ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2', 'Lag_1', 'Lag_2', 'Lag_3', 'Volatility', 'Momentum']\n",
      "Training data shape: (25072, 10), Testing data shape: (6269, 10)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/2tm7zw5j145975vwqn87sn100000gn/T/ipykernel_34175/947466608.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ticker_data['Lag_1'] = ticker_data['Adj Close'].shift(1)\n",
      "/var/folders/rb/2tm7zw5j145975vwqn87sn100000gn/T/ipykernel_34175/947466608.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ticker_data['Lag_2'] = ticker_data['Adj Close'].shift(2)\n",
      "/var/folders/rb/2tm7zw5j145975vwqn87sn100000gn/T/ipykernel_34175/947466608.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ticker_data['Lag_3'] = ticker_data['Adj Close'].shift(3)\n",
      "/var/folders/rb/2tm7zw5j145975vwqn87sn100000gn/T/ipykernel_34175/947466608.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ticker_data['Volatility'] = ticker_data['Adj Close'].rolling(window=7).std()\n",
      "/var/folders/rb/2tm7zw5j145975vwqn87sn100000gn/T/ipykernel_34175/947466608.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ticker_data['Momentum'] = ticker_data['Adj Close'].pct_change(periods=3)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 1112.5896 - val_loss: 7.0589 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 62.7711 - val_loss: 3.4052 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 59.8471 - val_loss: 2.4516 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 58.5273 - val_loss: 2.5769 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 57.6122 - val_loss: 2.2210 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 57.3054 - val_loss: 2.6393 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 57.9860 - val_loss: 1.2134 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 53.8144 - val_loss: 2.9088 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 53.4389 - val_loss: 1.5928 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 51.6688 - val_loss: 2.8243 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 51.4734 - val_loss: 3.2291 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 48.4543 - val_loss: 0.9878 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 48.2521 - val_loss: 4.4814 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 44.6621 - val_loss: 1.2716 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 46.2240 - val_loss: 1.0457 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 44.7235 - val_loss: 0.9869 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 43.8500 - val_loss: 0.9771 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 41.3077 - val_loss: 1.7679 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 41.4767 - val_loss: 1.1452 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 40.9124 - val_loss: 3.1326 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 39.9561 - val_loss: 1.5478 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 40.2271 - val_loss: 2.3263 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 39.9672 - val_loss: 1.5401 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 38.8329 - val_loss: 1.1109 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.5629 - val_loss: 2.2946 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 38.3353 - val_loss: 0.9478 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.6326 - val_loss: 1.2814 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.4401 - val_loss: 0.9993 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.5790 - val_loss: 1.1966 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.4673 - val_loss: 1.9006 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 35.7782 - val_loss: 2.6033 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 35.4037 - val_loss: 1.3024 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 35.0643 - val_loss: 1.1175 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.8753 - val_loss: 1.0617 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 38.2831 - val_loss: 1.1873 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 34.1629 - val_loss: 1.0743 - learning_rate: 2.5000e-04\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "Mean Squared Error (MSE): 0.95\n",
      "Mean Absolute Error (MAE): 0.63\n",
      "R-squared (R²): 1.00\n",
      "XOM model saved as 'models/model_XOM_lstm.h5'\n",
      "XOM scaler saved as 'models/scaler_XOM_lstm.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import sqlite3\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Path to SQLite database\n",
    "db_path = 'database/stocks_data.db'\n",
    "\n",
    "# Load data from SQLite\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    query = \"SELECT * FROM processed_stocks\"\n",
    "    data = pd.read_sql(query, conn)\n",
    "print(f\"Loaded processed data: {data.shape[0]} rows\")\n",
    "\n",
    "# Step 1: Set default ticker\n",
    "default_ticker = 'XOM'\n",
    "\n",
    "# Step 2: Filter data for the default ticker\n",
    "ticker_data = data[data['Ticker'] == default_ticker]\n",
    "print(f\"Loaded data for {default_ticker}: {ticker_data.shape[0]} rows\")\n",
    "\n",
    "# Define features and target\n",
    "features = ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2']\n",
    "target = 'Adj Close'\n",
    "\n",
    "X_raw = ticker_data[features]\n",
    "y_raw = ticker_data[target]\n",
    "\n",
    "# Apply the sliding window approach\n",
    "window_size = 10\n",
    "X, y = [], []\n",
    "for i in range(len(X_raw) - window_size):\n",
    "    X.append(X_raw.iloc[i:i+window_size, :].values)  # Take `window_size` rows of features\n",
    "    y.append(y_raw.iloc[i+window_size])  # Target is the next value after the window\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = []\n",
    "for i in range(X.shape[0]):\n",
    "    X_scaled.append(scaler.fit_transform(X[i]))  # Normalize each sliding window independently\n",
    "X_scaled = np.array(X_scaled)\n",
    "\n",
    "# Define features and target\n",
    "features = ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2']\n",
    "target = 'Adj Close'\n",
    "\n",
    "X = ticker_data[features]\n",
    "y = ticker_data[target]\n",
    "\n",
    "# Feature Engineering: Adding lagged variables, volatility, and rolling statistics\n",
    "ticker_data['Lag_1'] = ticker_data['Adj Close'].shift(1)\n",
    "ticker_data['Lag_2'] = ticker_data['Adj Close'].shift(2)\n",
    "ticker_data['Lag_3'] = ticker_data['Adj Close'].shift(3)\n",
    "ticker_data['Volatility'] = ticker_data['Adj Close'].rolling(window=7).std()\n",
    "ticker_data['Momentum'] = ticker_data['Adj Close'].pct_change(periods=3)\n",
    "\n",
    "# Drop NaN values introduced by feature engineering\n",
    "ticker_data = ticker_data.dropna()\n",
    "\n",
    "# Update features and target with engineered features\n",
    "engineered_features = features + ['Lag_1', 'Lag_2', 'Lag_3', 'Volatility', 'Momentum']\n",
    "X = ticker_data[engineered_features]\n",
    "y = ticker_data[target]\n",
    "print(f\"Updated feature set: {engineered_features}\")\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# No further reshaping needed as `X_scaled` is already in (samples, timesteps, features) format\n",
    "print(f\"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input for LSTM (samples, timesteps, features)\n",
    "X_train_scaled = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_scaled = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Build the optimized LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(1, X_train_scaled.shape[2])))\n",
    "model.add(Dropout(0.3))  # Add dropout for regularization\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))  # Fully connected output layer\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Callbacks for better training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=100,  # Start with 100 epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# Save the trained model and scaler\n",
    "model_filename = f'models/model_{default_ticker}_lstm.h5'\n",
    "scaler_filename = f'models/scaler_{default_ticker}_lstm.pkl'\n",
    "\n",
    "model.save(model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"{default_ticker} model saved as '{model_filename}'\")\n",
    "print(f\"{default_ticker} scaler saved as '{scaler_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import joblib\n",
    "# import sqlite3\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Path to SQLite database\n",
    "# db_path = 'database/stocks_data.db'\n",
    "\n",
    "# # Load data from SQLite\n",
    "# with sqlite3.connect(db_path) as conn:\n",
    "#     query = \"SELECT * FROM processed_stocks\"\n",
    "#     data = pd.read_sql(query, conn)\n",
    "# print(f\"Loaded processed data: {data.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Set default ticker\n",
    "# default_ticker = 'XOM'\n",
    "\n",
    "# # Step 2: Filter data for the default ticker\n",
    "# ticker_data = data[data['Ticker'] == default_ticker]\n",
    "# print(f\"Loaded data for {default_ticker}: {ticker_data.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define features and target\n",
    "# features = ['7-day MA', '14-day MA', 'Volatility', 'Lag_1', 'Lag_2']\n",
    "# target = 'Adj Close'\n",
    "\n",
    "# X = ticker_data[features]\n",
    "# y = ticker_data[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # Drop missing values\n",
    "# # data = data.dropna(subset=features + [target])\n",
    "# # X = data[features].values\n",
    "# # y = data[target].values\n",
    "\n",
    "# # # Normalize the features\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # # Reshape target to 2D for compatibility\n",
    "# # y = y.reshape(-1, 1)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Reshape input for LSTM (samples, timesteps, features)\n",
    "# # scaler.MinMaxScaler()\n",
    "# X_train_scaled = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "# X_test_scaled = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Build and train the LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, input_shape=(1, X_train_scaled.shape[2]), activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Early stopping for training\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_scaled, y_train, epochs=100, batch_size=42, validation_data=(X_test_scaled, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(\"Model Evaluation:\")\n",
    "# print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "# print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 8: Save the trained model and scaler\n",
    "# model_filename = f'models/model_{default_ticker}_lstm.pkl'\n",
    "# scaler_filename = f'models/scaler_{default_ticker}_lstm.pkl'\n",
    "\n",
    "# joblib.dump(model, model_filename)\n",
    "# joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# print(f\"{default_ticker} model saved as '{model_filename}'\")\n",
    "# print(f\"{default_ticker} scaler saved as '{scaler_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and scaler\n",
    "# try:\n",
    "#     lstm_model = joblib.load(model_file_path)\n",
    "#     scaler = joblib.load(scaler_file_path)\n",
    "#     model_and_scaler_status = \"LSTM model and scaler loaded successfully.\"\n",
    "# except Exception as e:\n",
    "#     model_and_scaler_status = f\"Error loading LSTM model or scaler: {e}\"\n",
    "\n",
    "# model_and_scaler_status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
